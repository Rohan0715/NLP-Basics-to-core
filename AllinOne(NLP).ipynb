{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e456ed9a",
   "metadata": {},
   "source": [
    "## Natural Language Processing (NLP)\n",
    "is a technology used to help computers understand human language. It’s used in many areas like social media, banking, and insurance.\n",
    "When you’re analyzing text (like a book or a tweet), you often start with a big chunk of text. Your job is to make sense of this text. For example, you might want to translate this text into another language, like Hindi.\n",
    "To do this, you need to break down the big task (translating the text) into smaller tasks. These smaller tasks might include splitting the text into sentences or words, or figuring out what a word means based on the words around it.\n",
    "In this course, you’ll learn about the different steps you usually take to go from a big chunk of text to understanding what the text means. This process can be split into three main parts.\n",
    "The course also includes a video that explains how to understand text. The video covers topics like semantic, syntactic, and lexical processing.\n",
    "Let’s take an example. Imagine you have a Wikipedia article. The article is just a bunch of characters that a computer can’t understand on its own. So, you need to follow certain steps to help the computer understand the text.\n",
    "The first step is\n",
    "## Lexical Processing.\n",
    "Here, you convert the raw text into words, sentences, or paragraphs, depending on what you need. For instance, if an email contains words like ‘lottery’, ‘prize’, and ‘luck’, it’s probably a spam email.\n",
    "However, just looking at the words isn’t enough for more complex tasks, like translating text. For example, the sentences ‘My cat ate its third meal’ and ‘My third cat ate its meal’ mean different things, but if you’re just looking at the words, you might think they mean the same thing.\n",
    "That’s where\n",
    "## Syntactic Processing\n",
    "comes in. This is the next step after Lexical Processing. Here, you try to understand the sentence better by looking at its structure or grammar. For example, you might look at who is doing an action and who is affected by it. This can help you tell the difference between sentences like ‘Ram thanked Shyam’ and ‘Shyam thanked Ram’.\n",
    "## Semantic Processing \n",
    "is the next step after Lexical and Syntactic processing in Natural Language Processing (NLP). It’s used when you’re building more advanced NLP applications, like language translation and chatbots.\n",
    "After Lexical and Syntactic processing, the machine might still not understand the meaning of the text. For example, it might not know that ‘PM’ and ‘Prime Minister’ mean the same thing. So, you need a way for the machine to learn this on its own.\n",
    "One way to do this is by looking at the words that usually appear around a word. If ‘PM’ and ‘Prime Minister’ often appear around similar words, you can assume they mean the same thing.\n",
    "The machine should also be able to understand other relationships between words. For example, it should know that ‘King’ and ‘Queen’ are related, and that ‘Queen’ is the female version of ‘King’. These words can be grouped under the word ‘Monarch’.\n",
    "Once the machine understands the meaning of words through Semantic Analysis, it can be used for various applications. <b>These applications need a complete understanding of the text, from the words (Lexical level), to the sentence structure (Syntactic level), to the meaning of the words (Semantic level)</b>.\n",
    "In simpler applications, only Lexical processing might be needed. But in most applications, Lexical and Semantic processing form the ‘preprocessing’ layer of the overall process.\n",
    "Now that you have a basic idea of how to analyze text and understand its meaning, the next segment will teach you how the text is stored on machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb89270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount $50^ and its encoded version b'\\xff\\xfe$\\x005\\x000\\x00^\\x00'\n",
      "The encoded version b'\\xff\\xfe$\\x005\\x000\\x00^\\x00' and its normal format $50^\n"
     ]
    }
   ],
   "source": [
    "amount=\"$50^\"\n",
    "amount_encoded = amount.encode('utf-16')#here i could have written utf-8 or ascii these are the wasy of encoding an element \n",
    "print(f\"The amount {amount} and its encoded version {amount_encoded}\")\n",
    "amount_decoded=amount_encoded.decode(\"utf-16\")\n",
    "print(f\"The encoded version {amount_encoded} and its normal format {amount_decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd0d0a2",
   "metadata": {},
   "source": [
    "## Regular Expressions – Quantifiers I\n",
    "From this segment onwards, you’ll learn about regular expressions. They are often known as regexes and are extremely powerful programming tools that can be used to extract features from the text, replace strings and perform other string manipulations. Being familiar with regular expressions is essential for becoming a text analytics expert.\n",
    "\n",
    "    A regular expression is a pattern or character collection used to find a text’s substrings\n",
    "Eg-\n",
    "Let’s say you want to extract all of the hashtags from a tweet. A hashtag follows a set pattern, consisting of a pound (‘#’) character followed by a string. Some hashtags include ‘#mumbai’, ‘#bengaluru’ and ‘#upgrad’. This work is readily accomplished by submitting this pattern as well as a tweet from which you wish to extract the pattern (in this example, the pattern is any string beginning with ‘#’)\n",
    "\n",
    "    Studying regular expressions entails learning how to recognise and define these patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e567c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9314e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "example=\"this consist of sentences which is random i even dont know what i am typing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41e03fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='this'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"this\",example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c74947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pattern(word,pattern):\n",
    "    if re.search(patter,word):\n",
    "        return re.search(pattern,word)\n",
    "    else:\n",
    "        return \"Not Found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ff02cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abb'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_pattern(\"abb\",\"ab*\")#zero or more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9737f131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='abbb'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_pattern(\"abbb\",\"ab+\")#one or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b301fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ab'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_pattern(\"abbb\",\"ab?\")#zero or one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d77e676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='abbb'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_pattern(\"abbb\",\"ab{1,3}\")# through this you could set the range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0269f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='j'>\n",
      "Not Found\n"
     ]
    }
   ],
   "source": [
    "#^ this symbol indicates the starting symbol thsi help us  to match the starting symbol\n",
    "# $ indicates the ending of the string \n",
    "print(find_pattern(\"james\",\"^j\"))\n",
    "print(find_pattern(\"rohan\",\"$n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2597da5",
   "metadata": {},
   "source": [
    "WildCard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c0c8e09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(4, 5), match='o'>\n"
     ]
    }
   ],
   "source": [
    "# . is an universal character it will match to anything\n",
    "print(find_pattern(\"hello\",\".$\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2afd1c",
   "metadata": {},
   "source": [
    "Character Sets []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee9d035a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='h'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here i could be giving an character in the square bracket where it would mathch each character from the set\n",
    "#here i could aslo mention the range by writing - \n",
    "find_pattern(\"rohan\",\"[a-r]\")\n",
    "find_pattern(\"hello\",\"[hehe]\")\n",
    "#internally it works on ascii values\n",
    "#sso suppose i write a-c it will understand the set as a has value of(ASCII) 70 it will start its search by 71,72,73...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba3536e",
   "metadata": {},
   "source": [
    "<b>Some popular Character sets</b>\n",
    "-  [aeiou] matches any vowel.\n",
    "-  [0-9] matches any digit.\n",
    "-  [^0-9] matches any non-digit character.\n",
    "-  [\\d\\s] matches any digit or whitespace character.\n",
    "-  [A-Z]\n",
    "-  [abcAbc]\n",
    "-  [A-z] it an case insensitive match\n",
    "\n",
    "\n",
    "<b>Meta Sequence</b>\n",
    "-  \\d: Matches any decimal digit; this is equivalent to the set [0-9].\n",
    "-  \\D: Matches any non-digit character; this is equivalent to the set [^0-9].\n",
    "-  \\s: Matches any whitespace character; this includes spaces, tabs, and newline characters.\n",
    "-  \\S: Matches any non-whitespace character.\n",
    "-  \\w: Matches any alphanumeric character; this includes letters, digits, and underscores.\n",
    "-  \\W: Matches any non-alphanumeric character.\n",
    "-  \\b: Matches a word boundary (position between a word character and a non-word character).\n",
    "-  \\B: Matches a non-word boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff7f80c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(5, 6), match=' '>\n",
      "<re.Match object; span=(0, 3), match='124'>\n",
      "<re.Match object; span=(0, 7), match='hey12He'>\n",
      "<re.Match object; span=(0, 1), match='H'>\n",
      "<re.Match object; span=(0, 1), match='1'>\n"
     ]
    }
   ],
   "source": [
    "print(find_pattern(\"Hello \",\"\\s+\"))# gives an match if one ore mroe then one white space is present\n",
    "print(find_pattern(\"124\",\"\\d+\"))# gives an match if any no. is present\n",
    "print(find_pattern(\"hey12He\",\"\\w+\"))\n",
    "print(find_pattern(\"Hey\",\"\\w\"))\n",
    "print(find_pattern(\"1ey\",\"\\w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9c0b8b",
   "metadata": {},
   "source": [
    "<b>Greedy vs Non-Greedy regex </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ab6624",
   "metadata": {},
   "source": [
    "<b>Greedy Matching</b>: Greedy matching attempts to match as much as possible while still satisfying the entire pattern. For example, if you use the pattern bat* on the string \"batsman\", it will match \"bat\" followed by as many \"t\" characters as possible, resulting in \"bat\" being matched entirely.\n",
    "\n",
    "\n",
    "<b>Non-greedy (Lazy) Matching</b>: Non-greedy matching, on the other hand, attempts to match as little as possible while still satisfying the entire pattern. For example, if you use the pattern bat*? on the string \"batsman\", it will match \"bat\" followed by as few \"t\" characters as possible, resulting in just \"ba\" being matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0d06bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(2, 7), match='abbbb'>\n"
     ]
    }
   ],
   "source": [
    "print(find_pattern(\"aaabbbbc\",\"ab{3,5}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79ca1701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(2, 6), match='abbb'>\n"
     ]
    }
   ],
   "source": [
    "print(find_pattern(\"aaabbbb\",\"ab{3,5}?\"))\n",
    "# when you want your search to stop at first constraint here it is 3 and dont go beyond that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82402ceb",
   "metadata": {},
   "source": [
    "### re.match(pattern, string):\n",
    "This function attempts to match the pattern at the beginning of the string. If the pattern matches, it returns a match object; otherwise, it returns None.\n",
    "### re.search(pattern, string):\n",
    "This function searches the entire string for a match to the pattern. It returns a match object if the pattern is found anywhere in the string; otherwise, it returns None.\n",
    "### re.findall(pattern, string):\n",
    "This function finds all occurrences of the pattern in the string and returns them as a list of strings. It does not return match objects but only the matched substrings.\n",
    "### re.sub(pattern, repl, string):\n",
    "This function replaces occurrences of the pattern in the string with the replacement string repl. It returns the modified string.\n",
    "### re.split(pattern, string):\n",
    "This function splits the string into substrings based on the occurrences of the pattern and returns them as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c32c09d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found at the beginning: The\n",
      "Match found anywhere: fox\n",
      "All words with 4 characters: ['over', 'lazy']\n",
      "Modified text: The quick red fox jumps over the lazy dog\n",
      "Words in the text: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample string\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# 1. re.match()\n",
    "match_obj = re.match(r\"The\", text)\n",
    "if match_obj:\n",
    "    print(\"Match found at the beginning:\", match_obj.group())\n",
    "else:\n",
    "    print(\"No match found\")\n",
    "\n",
    "# 2. re.search()\n",
    "search_obj = re.search(r\"fox\", text)\n",
    "if search_obj:\n",
    "    print(\"Match found anywhere:\", search_obj.group())\n",
    "else:\n",
    "    print(\"No match found\")\n",
    "\n",
    "# 3. re.findall()\n",
    "matches = re.findall(r\"\\b\\w{4}\\b\", text)\n",
    "print(\"All words with 4 characters:\", matches)\n",
    "\n",
    "# 4. re.sub()\n",
    "modified_text = re.sub(r\"brown\", \"red\", text)\n",
    "print(\"Modified text:\", modified_text)\n",
    "\n",
    "# 5. re.split()\n",
    "words = re.split(r\"\\s\", text)\n",
    "print(\"Words in the text:\", words)\n",
    "\n",
    "#usage of group function:\n",
    "pattern=\"/\\b\\d{3}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98386059",
   "metadata": {},
   "source": [
    "<b>An practical implementation of regex</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a701b90e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['555-123-4567']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "group= (\"(555) 123-4567\",\"555-123-4567\",\"5551234567\")\n",
    "information=\"So here is my phone no.555-123-4567 and my gmail rohanpatel.737797045@gmail.com\"\n",
    "pattern=\"\\(\\d{3}\\)\\W?\\s*\\d{3}\\W?\\s?\\d{4}|\\d{3}\\W?\\d{3}\\W?\\d{4}|\\d{10}\"\n",
    "pattern2=\"\\(\\d{3}\\)\\W?\\s*\\d{3}\\W?\\s?\\d{4}|\\d{3}\\W?\\d{3}\\W?\\d{4}|\\d{10}|[a-zA-Z0-9]*\\.[0.9]*\\.@gmail\\.com\"\n",
    "# for x in group:\n",
    "#     print(re.findall(pattern,x))\n",
    "print(re.findall(pattern2,information))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71b419cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Code: 555, Exchange Code: 123, Subscriber Number: 4567\n",
      "Area Code: 555, Exchange Code: 123, Subscriber Number: 4567\n",
      "Area Code: 555, Exchange Code: 123, Subscriber Number: 4567\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "group = (\"(555) 123-4567\", \"555-123-4567\", \"5551234567\")\n",
    "pattern = r\"(\\d{3})\\W?\\s*(\\d{3})\\W?\\s?(\\d{4})\"\n",
    "\n",
    "for phone_number in group:\n",
    "    match = re.search(pattern, phone_number)\n",
    "    if match:\n",
    "        area_code = match.group(1)\n",
    "        exchange_code = match.group(2)\n",
    "        subscriber_number = match.group(3)\n",
    "        print(f\"Area Code: {area_code}, Exchange Code: {exchange_code}, Subscriber Number: {subscriber_number}\")\n",
    "    else:\n",
    "        print(f\"No match found for '{phone_number}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4388b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone Numbers: ['555-123-4567']\n",
      "Gmail Addresses: ['rohanpatel.737797045@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "information = \"So here is my phone no.555-123-4567 and my gmail rohanpatel.737797045@gmail.com\"\n",
    "pattern_phone = r\"\\(\\d{3}\\)\\W?\\s*\\d{3}\\W?\\s?\\d{4}|\\d{3}\\W?\\d{3}\\W?\\d{4}|\\d{10}\"\n",
    "pattern_gmail = r\"[a-zA-Z0-9]+\\.[0-9]+@gmail\\.com\"\n",
    "phone_numbers = re.findall(pattern_phone, information)\n",
    "gmail_addresses = re.findall(pattern_gmail, information)\n",
    "\n",
    "print(\"Phone Numbers:\", phone_numbers)\n",
    "print(\"Gmail Addresses:\", gmail_addresses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "407e4e92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    3.7.4                         \n",
      "Location         C:\\Users\\rashi\\anaconda\\lib\\site-packages\\spacy\n",
      "Platform         Windows-10-10.0.22621-SP0     \n",
      "Python version   3.9.13                        \n",
      "Pipelines                                      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy info\n",
    "# this command shows the information regarding the python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4835aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rashi\\anaconda\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello this is an example of how we can use Tokenization.', 'Here it ends']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentence=\"Hello this is an example of how we can use Tokenization. Here it ends\"\n",
    "sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e6d9940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'how',\n",
       " 'we',\n",
       " 'can',\n",
       " 'use',\n",
       " 'Tokenization',\n",
       " '.',\n",
       " 'Here',\n",
       " 'it',\n",
       " 'ends']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6c6978",
   "metadata": {},
   "source": [
    "## Design Philosophy:\n",
    "-  NLTK: NLTK is a comprehensive library for natural language processing (NLP) written in Python. It provides a wide range of tools and modules for tasks such as tokenization, stemming, tagging, parsing, and classification. NLTK is designed to be a learning tool, making it suitable for educational purposes and experimentation.\n",
    "-  SpaCy: SpaCy is a modern and efficient library for NLP written in Python and Cython. It's designed to be fast, easy to use, and production-ready. SpaCy focuses on providing state-of-the-art performance and capabilities for real-world NLP applications.\n",
    "## Ease of Use:\n",
    "-  NLTK: NLTK is more flexible and customizable, allowing users to build NLP pipelines from scratch and experiment with different algorithms and techniques. It provides a wide range of modules and functions, but this flexibility can make it more complex for beginners.\n",
    "-  SpaCy: SpaCy is designed to be simple and intuitive, with pre-trained models and streamlined APIs that make common NLP tasks easy to perform. It provides out-of-the-box support for tasks such as tokenization, named entity recognition (NER), part-of-speech (POS) tagging, and dependency parsing, making it more beginner-friendly and efficient for many use cases.\n",
    "## Performance:\n",
    "-  NLTK: NLTK provides a rich set of algorithms and tools, but it may not be as efficient or performant as SpaCy, especially for large-scale or production-grade NLP tasks.\n",
    "-  SpaCy: SpaCy is known for its speed and efficiency, thanks to its optimized implementation and use of Cython. It's designed to handle large volumes of text and perform complex NLP tasks quickly, making it well-suited for production environments and real-time applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a29e57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download()# this will open an terminal of all the packages that nltk \n",
    "#have you could use this to see which which are package are used for what and what are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7abe1825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version) # this command is used to check the version of python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e44085",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai as it costs only 2$ per plate.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92dcd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[0]# and here we could apply more function to it to identifie what it is in particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example=doc[0]#here i have made an object of first token\n",
    "example.is_alpha# her eafter typing is_ we could tap tab to access more function to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff5f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "num=doc[11]\n",
    "num.is_digit #but before we could use tab ew have run that object so that system could recognise that varibale \n",
    "#then only you can use that tab to get acces to all the avaialabe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a1c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ForNlp.txt\") as f:\n",
    "    text=f.readlines()\n",
    "text\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afe58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f6705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "hello=word_tokenize(text[0])\n",
    "print(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b376e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hey=\" \".join(hello)\n",
    "print(hey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd483fc",
   "metadata": {},
   "source": [
    "#### Reading and extracting gmails and phone numbers from build in libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Dummie.txt\") as f:\n",
    "    text=f.readlines();\n",
    "type(text)\n",
    "# now it is in the from of list so to convert all the element of list into one singnal line \n",
    "textz=\" \".join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13783c60",
   "metadata": {},
   "source": [
    "## Steps to use spacy\n",
    "1. import language in spacy that which language you want to get engaged by using <b>nlp=spacy.blank(\"en\",\"hi\")</b> and make it store in an variable so that it could be used as an object\n",
    "2. Put setence under the object you have created like <b>nlp(\"here is the sentence\")</b>\n",
    "3. you can perform operation on it by taking the build in function provided by the spacy librarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f403f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.blank(\"en\")\n",
    "docsec=nlp(textz)\n",
    "emails=[]\n",
    "for token in docsec:\n",
    "    if token.like_email:\n",
    "        emails.append(token)\n",
    "num=1\n",
    "for x in emails:\n",
    "    print(f\"This is the {num} email {x} \")\n",
    "    num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c58e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_text=\"का हिन्दी अनुवाद | कोलिन्स अंग्रेज़ी-हिन्दी शब्दकोश\"\n",
    "with open(\"hindifile.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(doc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(\"here is an example of an sentence where it includes many thing which cannot be compiled at once\")\n",
    "token_doc=[token.text for token in doc]\n",
    "# here if i didnt add token.text in front of token we could get an object called spacy.tokens.token.Token \n",
    "# if i add token.text in token in the above loop we will get an string as an element in token_doc list\n",
    "print(token_doc)\n",
    "type(token_doc[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be98f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppose i have sentence where gimme is defiend in it and i want it to be calles as give me in tokens format\n",
    "#so what i can do is i can introduce an special case justfor gimme so that whenever tokenization performs it should\n",
    "# return give and me \n",
    "from spacy.symbols import ORTH\n",
    "nlp.tokenizer.add_special_case(\"gimme\",[{ORTH:\"gim\"},{ORTH:\"me\"}])\n",
    "#add_special_case() takes exactly 2 positional arguments \n",
    "nlp.tokenizer.add_special_case(\"sandwich\",[{ORTH:\"sand\"},{ORTH:\"wich\"}])\n",
    "example=nlp(\"gimme a sandwich\")\n",
    "token=[token.text for token in example]\n",
    "print(token)\n",
    "# but remember coul only spit one word in  2  words but can never update the character\n",
    "# it can just split the character in two but not modifie it try putting give in place of gim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66f1e05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#and for spliting sentence from the given text  we need to add an componet which will basically teach\n",
    "# tokenizer to split anything  using differnet rules\n",
    "# so for it we have add an pipeline is na basically different attributess or boundary whcih are firt checked\n",
    "# then it is tokenized\n",
    "doctor=nlp(\"Dr. Ambedakar Centrally Sponsored Scheme of Post-Matric Scholarships for the Economically Backward Class (EBC) Students\\\" is a Scholarship Scheme by the Department of Social Justice and Empowerment, Ministry of Social Justice and Empowerment.\")\n",
    "for token in doctor.sents:\n",
    "    print(token)\n",
    "\n",
    "# this is the error we will get "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e01992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c0624",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24706e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names\n",
    "#here suppose we have added an component with tokenizer to make it capable of spliting sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d0c0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will able to pefrom the above operation\n",
    "doctor=nlp(\"\")\n",
    "for token in doctor.sents:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aef503",
   "metadata": {},
   "source": [
    "\n",
    "![Alt text](2.png)\n",
    "### when you used spacy.blank you basically create an tokenizer with blank pipe \n",
    "![Alt text](1.png)\n",
    "### and here you will get an pipe which are fill with attributes and rules through  tokenizing will work\n",
    "### Pipeline are basically a set of components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32851758",
   "metadata": {},
   "source": [
    "## Language Processing  pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5841e239",
   "metadata": {},
   "source": [
    "Overview of spaCy Language Processing Pipeline:\n",
    "\n",
    "-  Components: Tokenization, Part-of-Speech Tagging, Lemmatization, Named Entity Recognition (NER).\n",
    "-  Importance: Essential for NLP tasks, enhances text processing capabilities.\n",
    "\n",
    "Customization of spaCy Pipeline:\n",
    "\n",
    "-  Blank Pipeline Customization: Users can include only necessary components for their specific needs.\n",
    "-  Differences: Pre-trained pipelines vs. blank pipelines, flexibility vs. specificity.\n",
    "\n",
    "Accessing NLP Tools:\n",
    "\n",
    "-  APIs: Provide quick access for implementation without deep knowledge.\n",
    "-  Importance of Understanding: Key components like part of speech and entities aid in NLP analysis and processing.\n",
    "\n",
    "Named Entity Recognition (NER):\n",
    "\n",
    "-  Customization: Allows recognition of entities in text, enhancing language analysis.\n",
    "-  Variation in Pipelines: Different pipelines offer varying language support and components for entity recognition.\n",
    "\n",
    "Utilizing 'displacy' Module:\n",
    "\n",
    "-  Visual Entity Display: Helps understand entity types like organization, money, and people.\n",
    "\n",
    "Importance of Language Pipelines:\n",
    "\n",
    "-  Obtaining POS, Lemma, and Entity Recognition: Crucial for text processing and comprehension.\n",
    "-  Differences between Blank and Trained Pipelines: Highlighted importance of customization for specific tasks.\n",
    "\n",
    "Customizing Blank Pipeline:\n",
    "\n",
    "-  Adding Custom Components: Enhances entity recognition for specific tasks in English sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d4dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5828241f",
   "metadata": {},
   "source": [
    "!: This symbol indicates that the command is being executed in the shell or command line interface rather than in Python code.(that whaterver it follows it is an shell command not an python code)\n",
    "\n",
    "python: This is the command used to execute Python code in the shell.\n",
    "\n",
    "-m: This flag stands for \"module\" and tells Python to run the specified module as a script. It's used when you want to execute a module as a standalone script, without needing to know the exact location of the module file.\n",
    "\n",
    "spacy: This is the module or package name that we want to run.\n",
    "\n",
    "download: This is a command provided by the spacy module to download resources such as language models or data files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b77cb43e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 10.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (63.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.24.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.64.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2020.6.20)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\rashi\\anaconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\rashi\\anaconda\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "#command to download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ad13e39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rashi\\anaconda\\python.exe\n"
     ]
    }
   ],
   "source": [
    "#this command shows where the python is installed and in which enviorment`\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "262b4589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07d0c3e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1f7a9dc6640>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1f7a9dc65e0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1f7a9ca4dd0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1f7a9cdd680>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1f7a9f29900>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1f7a9da7350>)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e8b85f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it| PRON | it\n",
      "means| VERB | mean\n",
      "that| SCONJ | that\n",
      "the| DET | the\n",
      "command| NOUN | command\n",
      "is| AUX | be\n",
      "being| AUX | be\n",
      "executed| VERB | execute\n",
      "in| ADP | in\n",
      "the| DET | the\n",
      "shell| NOUN | shell\n",
      "or| CCONJ | or\n",
      "command| NOUN | command\n",
      "line| NOUN | line\n",
      "interface| NOUN | interface\n",
      "rather| ADV | rather\n",
      "than| ADP | than\n",
      "being| AUX | be\n",
      "interpreted| VERB | interpret\n",
      "as| ADP | as\n",
      "Python| PROPN | Python\n",
      "code| NOUN | code\n",
      ".| PUNCT | .\n"
     ]
    }
   ],
   "source": [
    "one=nlp(\"it means that the command is being executed in the shell or command line interface rather than being interpreted as Python code.\")\n",
    "for token in one:\n",
    "    print(f\"{token}| {token.pos_} | {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a4023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla | ORG | Companies, agencies, institutions, etc.\n",
      "more than $500 million | MONEY | Monetary values, including unit\n",
      "this year | DATE | Absolute or relative dates or periods\n",
      "Elon Musk | PERSON | People, including fictional\n",
      "Friday | DATE | Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "#an example of NER(NAMED ENTITY RECOGNISE)\n",
    "two=nlp(\"Tesla will spend more than $500 million this year to expand its fast-charging network, CEO Elon Musk said on Friday\")\n",
    "for token in two.ents:\n",
    "    print(token.text,\"|\",token.label_,\"|\",spacy.explain(token.label_))\n",
    "    #label will tell you baout ner of an sentence like what does the part of the sentence represent wheter its an organisation or an geoloigcal place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d22787f",
   "metadata": {},
   "source": [
    "## An way to visualise NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83ff5da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"3b14211da9214a8bb848231dde5950ec-0\" class=\"displacy\" width=\"3900\" height=\"837.0\" direction=\"ltr\" style=\"max-width: none; height: 837.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Tesla</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">will</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">spend</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">more</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">than</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">500</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">million</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">this</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">year</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">expand</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">its</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">fast-</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">charging</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">network,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">CEO</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">Elon</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">Musk</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">said</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"747.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">Friday</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-0\" stroke-width=\"2px\" d=\"M70,702.0 C70,527.0 370.0,527.0 370.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,704.0 L62,692.0 78,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-1\" stroke-width=\"2px\" d=\"M245,702.0 C245,614.5 365.0,614.5 365.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,704.0 L237,692.0 253,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-2\" stroke-width=\"2px\" d=\"M420,702.0 C420,2.0 3375.0,2.0 3375.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,704.0 L412,692.0 428,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-3\" stroke-width=\"2px\" d=\"M595,702.0 C595,352.0 1255.0,352.0 1255.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,704.0 L587,692.0 603,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-4\" stroke-width=\"2px\" d=\"M770,702.0 C770,439.5 1250.0,439.5 1250.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,704.0 L762,692.0 778,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-5\" stroke-width=\"2px\" d=\"M945,702.0 C945,527.0 1245.0,527.0 1245.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,704.0 L937,692.0 953,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-6\" stroke-width=\"2px\" d=\"M1120,702.0 C1120,614.5 1240.0,614.5 1240.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,704.0 L1112,692.0 1128,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-7\" stroke-width=\"2px\" d=\"M420,702.0 C420,264.5 1260.0,264.5 1260.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1260.0,704.0 L1268.0,692.0 1252.0,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-8\" stroke-width=\"2px\" d=\"M1470,702.0 C1470,614.5 1590.0,614.5 1590.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,704.0 L1462,692.0 1478,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-9\" stroke-width=\"2px\" d=\"M420,702.0 C420,177.0 1615.0,177.0 1615.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1615.0,704.0 L1623.0,692.0 1607.0,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-10\" stroke-width=\"2px\" d=\"M1820,702.0 C1820,614.5 1940.0,614.5 1940.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,704.0 L1812,692.0 1828,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-11\" stroke-width=\"2px\" d=\"M420,702.0 C420,89.5 1970.0,89.5 1970.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1970.0,704.0 L1978.0,692.0 1962.0,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-12\" stroke-width=\"2px\" d=\"M2170,702.0 C2170,527.0 2645.0,527.0 2645.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2170,704.0 L2162,692.0 2178,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-13\" stroke-width=\"2px\" d=\"M2345,702.0 C2345,614.5 2465.0,614.5 2465.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2345,704.0 L2337,692.0 2353,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-14\" stroke-width=\"2px\" d=\"M2520,702.0 C2520,614.5 2640.0,614.5 2640.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2520,704.0 L2512,692.0 2528,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-15\" stroke-width=\"2px\" d=\"M1995,702.0 C1995,439.5 2650.0,439.5 2650.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2650.0,704.0 L2658.0,692.0 2642.0,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-16\" stroke-width=\"2px\" d=\"M2870,702.0 C2870,527.0 3170.0,527.0 3170.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2870,704.0 L2862,692.0 2878,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-17\" stroke-width=\"2px\" d=\"M3045,702.0 C3045,614.5 3165.0,614.5 3165.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3045,704.0 L3037,692.0 3053,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-18\" stroke-width=\"2px\" d=\"M3220,702.0 C3220,614.5 3340.0,614.5 3340.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3220,704.0 L3212,692.0 3228,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-19\" stroke-width=\"2px\" d=\"M3395,702.0 C3395,614.5 3515.0,614.5 3515.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3515.0,704.0 L3523.0,692.0 3507.0,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-3b14211da9214a8bb848231dde5950ec-0-20\" stroke-width=\"2px\" d=\"M3570,702.0 C3570,614.5 3690.0,614.5 3690.0,702.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-3b14211da9214a8bb848231dde5950ec-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3690.0,704.0 L3698.0,692.0 3682.0,692.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(two,style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "187e9a66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " will spend \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    more than $500 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    this year\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " to expand its fast-charging network, CEO \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Elon Musk\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " said on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Friday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(two,style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "682ad778",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package spacy.displacy in spacy:\n",
      "\n",
      "NAME\n",
      "    spacy.displacy - spaCy's built in visualization suite for dependencies and named entities.\n",
      "\n",
      "DESCRIPTION\n",
      "    DOCS: https://spacy.io/api/top-level#displacy\n",
      "    USAGE: https://spacy.io/usage/visualizers\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    render\n",
      "    templates\n",
      "\n",
      "FUNCTIONS\n",
      "    app(environ, start_response)\n",
      "    \n",
      "    get_doc_settings(doc: spacy.tokens.doc.Doc) -> Dict[str, Any]\n",
      "    \n",
      "    parse_deps(orig_doc: Union[spacy.tokens.doc.Doc, spacy.tokens.span.Span], options: Dict[str, Any] = {}) -> Dict[str, Any]\n",
      "        Generate dependency parse in {'words': [], 'arcs': []} format.\n",
      "        \n",
      "        orig_doc (Union[Doc, Span]): Document to parse.\n",
      "        options (Dict[str, Any]): Dependency parse specific visualisation options.\n",
      "        RETURNS (dict): Generated dependency parse keyed by words and arcs.\n",
      "    \n",
      "    parse_ents(doc: spacy.tokens.doc.Doc, options: Dict[str, Any] = {}) -> Dict[str, Any]\n",
      "        Generate named entities in [{start: i, end: i, label: 'label'}] format.\n",
      "        \n",
      "        doc (Doc): Document to parse.\n",
      "        options (Dict[str, Any]): NER-specific visualisation options.\n",
      "        RETURNS (dict): Generated entities keyed by text (original text) and ents.\n",
      "    \n",
      "    parse_spans(doc: spacy.tokens.doc.Doc, options: Dict[str, Any] = {}) -> Dict[str, Any]\n",
      "        Generate spans in [{start_token: i, end_token: i, label: 'label'}] format.\n",
      "        \n",
      "        doc (Doc): Document to parse.\n",
      "        options (Dict[str, any]): Span-specific visualisation options.\n",
      "        RETURNS (dict): Generated span types keyed by text (original text) and spans.\n",
      "    \n",
      "    render(docs: Union[Iterable[Union[spacy.tokens.doc.Doc, spacy.tokens.span.Span, dict]], spacy.tokens.doc.Doc, spacy.tokens.span.Span, dict], style: str = 'dep', page: bool = False, minify: bool = False, jupyter: Optional[bool] = None, options: Dict[str, Any] = {}, manual: bool = False) -> str\n",
      "        Render displaCy visualisation.\n",
      "        \n",
      "        docs (Union[Iterable[Union[Doc, Span, dict]], Doc, Span, dict]]): Document(s) to visualise.\n",
      "            a 'dict' is only allowed here when 'manual' is set to True\n",
      "        style (str): Visualisation style, 'dep' or 'ent'.\n",
      "        page (bool): Render markup as full HTML page.\n",
      "        minify (bool): Minify HTML markup.\n",
      "        jupyter (bool): Override Jupyter auto-detection.\n",
      "        options (dict): Visualiser-specific options, e.g. colors.\n",
      "        manual (bool): Don't parse `Doc` and instead expect a dict/list of dicts.\n",
      "        RETURNS (str): Rendered SVG or HTML markup.\n",
      "        \n",
      "        DOCS: https://spacy.io/api/top-level#displacy.render\n",
      "        USAGE: https://spacy.io/usage/visualizers\n",
      "    \n",
      "    serve(docs: Union[Iterable[spacy.tokens.doc.Doc], spacy.tokens.doc.Doc], style: str = 'dep', page: bool = True, minify: bool = False, options: Dict[str, Any] = {}, manual: bool = False, port: int = 5000, host: str = '0.0.0.0', auto_select_port: bool = False) -> None\n",
      "        Serve displaCy visualisation.\n",
      "        \n",
      "        docs (list or Doc): Document(s) to visualise.\n",
      "        style (str): Visualisation style, 'dep' or 'ent'.\n",
      "        page (bool): Render markup as full HTML page.\n",
      "        minify (bool): Minify HTML markup.\n",
      "        options (dict): Visualiser-specific options, e.g. colors.\n",
      "        manual (bool): Don't parse `Doc` and instead expect a dict/list of dicts.\n",
      "        port (int): Port to serve visualisation.\n",
      "        host (str): Host to serve visualisation.\n",
      "        auto_select_port (bool): Automatically select a port if the specified port is in use.\n",
      "        \n",
      "        DOCS: https://spacy.io/api/top-level#displacy.serve\n",
      "        USAGE: https://spacy.io/usage/visualizers\n",
      "    \n",
      "    set_render_wrapper(func: Callable[[str], str]) -> None\n",
      "        Set an optional wrapper function that is called around the generated\n",
      "        HTML markup on displacy.render. This can be used to allow integration into\n",
      "        other platforms, similar to Jupyter Notebooks that require functions to be\n",
      "        called around the HTML. It can also be used to implement custom callbacks\n",
      "        on render, or to embed the visualization in a custom page.\n",
      "        \n",
      "        func (callable): Function to call around markup before rendering it. Needs\n",
      "            to take one argument, the HTML markup, and should return the desired\n",
      "            output of displacy.render.\n",
      "\n",
      "DATA\n",
      "    Any = typing.Any\n",
      "        Special type indicating an unconstrained type.\n",
      "        \n",
      "        - Any is compatible with every type.\n",
      "        - Any assumed to have all methods.\n",
      "        - All values assumed to be instances of Any.\n",
      "        \n",
      "        Note that all the above statements are true from the point of view of\n",
      "        static type checkers. At runtime, Any should not be used with instance\n",
      "        or class checks.\n",
      "    \n",
      "    Callable = typing.Callable\n",
      "        Callable type; Callable[[int], str] is a function of (int) -> str.\n",
      "        \n",
      "        The subscription syntax must always be used with exactly two\n",
      "        values: the argument list and the return type.  The argument list\n",
      "        must be a list of types or ellipsis; the return type must be a single type.\n",
      "        \n",
      "        There is no syntax to indicate optional or keyword arguments,\n",
      "        such function types are rarely used as callback types.\n",
      "    \n",
      "    Dict = typing.Dict\n",
      "        A generic version of dict.\n",
      "    \n",
      "    Iterable = typing.Iterable\n",
      "        A generic version of collections.abc.Iterable.\n",
      "    \n",
      "    Optional = typing.Optional\n",
      "        Optional type.\n",
      "        \n",
      "        Optional[X] is equivalent to Union[X, None].\n",
      "    \n",
      "    RENDER_WRAPPER = None\n",
      "    Union = typing.Union\n",
      "        Union type; Union[X, Y] means either X or Y.\n",
      "        \n",
      "        To define a union, use e.g. Union[int, str].  Details:\n",
      "        - The arguments must be types and there must be at least one.\n",
      "        - None as an argument is a special case and is replaced by\n",
      "          type(None).\n",
      "        - Unions of unions are flattened, e.g.::\n",
      "        \n",
      "            Union[Union[int, str], float] == Union[int, str, float]\n",
      "        \n",
      "        - Unions of a single argument vanish, e.g.::\n",
      "        \n",
      "            Union[int] == int  # The constructor actually returns int\n",
      "        \n",
      "        - Redundant arguments are skipped, e.g.::\n",
      "        \n",
      "            Union[int, str, int] == Union[int, str]\n",
      "        \n",
      "        - When comparing unions, the argument order is ignored, e.g.::\n",
      "        \n",
      "            Union[int, str] == Union[str, int]\n",
      "        \n",
      "        - You cannot subclass or instantiate a union.\n",
      "        - You can use Optional[X] as a shorthand for Union[X, None].\n",
      "\n",
      "FILE\n",
      "    c:\\users\\rashi\\anaconda\\lib\\site-packages\\spacy\\displacy\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(displacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51918940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. | \n",
      "Strange | PERSON\n",
      "hey | \n",
      "yeah | \n",
      ". | \n",
      "Hulk | \n",
      "loves | \n",
      "chat | \n",
      "from | \n",
      "delhi | GPE\n"
     ]
    }
   ],
   "source": [
    "nlp_one=spacy.blank(\"en\")\n",
    "nlp_one.add_pipe(\"ner\",source=nlp)\n",
    "hey=nlp_one(\"Dr. Strange hey yeah. Hulk loves chat from delhi\")\n",
    "for token in hey:\n",
    "    print(f\"{token} | {token.ent_type_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ce0b3",
   "metadata": {},
   "source": [
    "## Stemimng and lemmatization\n",
    "### Stemming\n",
    "used fixed rules such as remove able ,ing to derive its base word\n",
    "### Lemmatization\n",
    "where lemmatization is used to bring the word coomes to there root word with knowledge of language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e496d022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rashi\\anaconda\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running|run\n",
      "cats|cat\n",
      "jumped|jump\n",
      "studying|studi\n",
      "friendly|friendli\n",
      "eating|eat\n",
      "walked|walk\n",
      "swimming|swim\n",
      "houses|hous\n",
      "faster|faster\n"
     ]
    }
   ],
   "source": [
    "# for applying stemming we have to use nltk librarie cause in spacy there isnt a function which can perform stemming\n",
    "import nltk \n",
    "import spacy\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import PorterStemmer# Porter stemmer is an class so to use its function we have create an object of it first\n",
    "stemmer= PorterStemmer()\n",
    "words=[\"running\",\"cats\",\"jumped\",\"studying\",\"friendly\",\"eating\",\"walked\",\"swimming\",\"houses\",\"faster\"]\n",
    "stemmed_words=[]\n",
    "for word in words:\n",
    "    stemmed_words.append(stemmer.stem(word))\n",
    "for worde,wordr in zip(words,stemmed_words):\n",
    "    print(f\"{worde}|{wordr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a35c1f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running | running\n",
      "cats | cat\n",
      "jumped | jumped\n",
      "studying | studying\n",
      "friendly | friendly\n",
      "eating | eating\n",
      "walked | walked\n",
      "swimming | swimming\n",
      "houses | house\n",
      "faster | faster\n"
     ]
    }
   ],
   "source": [
    "#performing lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "words=[\"running\",\"cats\",\"jumped\",\"studying\",\"friendly\",\"eating\",\"walked\",\"swimming\",\"houses\",\"faster\"]\n",
    "for word in words:\n",
    "    print(f\"{word} | {lemmatizer.lemmatize(word)}\")\n",
    "#this dosent give the proper result as we expected cause we have to give proper pos tags\n",
    "# now lets use spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b3e6f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running | run\n",
      "cats | cat\n",
      "jumped | jump\n",
      "studying | study\n",
      "friendly | friendly\n",
      "eating | eating\n",
      "walked | walk\n",
      "swimming | swimming\n",
      "houses | house\n",
      "faster | fast\n",
      "better | well\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "hey=nlp(\"running cats jumped studying friendly eating walked swimming houses faster better\")\n",
    "for word in hey:\n",
    "    print(f\"{word} | {word.lemma_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9823242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bruh | Brother\n",
      "means | mean\n",
      "brother | brother\n",
      "and | and\n",
      "bro | Brother\n",
      "also | also\n",
      "means | mean\n",
      "brother | brother\n"
     ]
    }
   ],
   "source": [
    "       \n",
    "#suppose there are some words which you knwo mean sthe same but model dosent so you can add an special rule too\n",
    "nlp.pipe_names\n",
    "#attribute_ruler so this is something which assigns an attribute to the word so you have customize it get the result\n",
    "ar=nlp.get_pipe(\"attribute_ruler\")\n",
    "\n",
    "ar.add([[{\"TEXT\":\"bro\"}],[{\"TEXT\":\"bruh\"}]],{\"LEMMA\":\"Brother\"})\n",
    "hey=nlp(\"bruh means brother and bro also means brother\")\n",
    "for word in hey:\n",
    "    print(f\"{word} | {word.lemma_}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606000a0",
   "metadata": {},
   "source": [
    "### POS(PART OF SPEECH) BY SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db6eb979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. | PROPN | proper noun |96 | NNP | noun, proper singular\n",
      "Strange | PROPN | proper noun |96 | NNP | noun, proper singular\n",
      "hey | INTJ | interjection |91 | UH | interjection\n",
      "yeah | INTJ | interjection |91 | UH | interjection\n",
      ". | PUNCT | punctuation |97 | . | punctuation mark, sentence closer\n",
      "Hulk | PROPN | proper noun |96 | NNP | noun, proper singular\n",
      "loves | VERB | verb |100 | VBZ | verb, 3rd person singular present\n",
      "chat | NOUN | noun |92 | NN | noun, singular or mass\n",
      "from | ADP | adposition |85 | IN | conjunction, subordinating or preposition\n",
      "delhi | ADV | adverb |86 | RB | adverb\n",
      ". | PUNCT | punctuation |97 | . | punctuation mark, sentence closer\n",
      "And | CCONJ | coordinating conjunction |89 | CC | conjunction, coordinating\n",
      "hulk | NOUN | noun |92 | NN | noun, singular or mass\n",
      "made | VERB | verb |100 | VBD | verb, past tense\n",
      "it | PRON | pronoun |95 | PRP | pronoun, personal\n",
      "to | ADP | adposition |85 | IN | conjunction, subordinating or preposition\n",
      "the | DET | determiner |90 | DT | determiner\n",
      "very | ADJ | adjective |84 | JJ | adjective (English), other noun-modifier (Chinese)\n",
      "end | NOUN | noun |92 | NN | noun, singular or mass\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "sentence=nlp(\"Dr. Strange hey yeah. Hulk loves chat from delhi.And hulk made it to the very end\")\n",
    "for word in sentence:\n",
    "    #here i would also include the tag cause through that we could get the tense of the word used in sentence\n",
    "    print(f\"{word} | {word.pos_} | {spacy.explain(word.pos_)} |{word.pos} | {word.tag_} | {spacy.explain(word.tag_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1ef836",
   "metadata": {},
   "source": [
    "-  {word.pos_}: This placeholder is replaced with the textual representation of the part-of-speech (POS) tag of the word.\n",
    "-  {spacy.explain(word.pos_)}: This placeholder is replaced with the explanation of the POS tag provided by spaCy.\n",
    "-  {word.pos}: This placeholder is replaced with the numerical representation of the POS tag of the word.\n",
    "-  {word.tag_}: This placeholder is replaced with the textual representation of the detailed POS tag (also known as the fine-grained tag) of the word.\n",
    "-  {spacy.explain(word.tag_)}: This placeholder is replaced with the explanation of the detailed POS tag provided by spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a1d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_text=\"\"\"Microsoft Corp. today announced the following results for the quarter ended December 31, 2021, as compared to the corresponding period of last fiscal year:\n",
    "·         Revenue was $51.7 billion and increased 20%\n",
    "·         Operating income was $22.2 billion and increased 24%\n",
    "·         Net income was $18.8 billion and increased 21%\n",
    "·         Diluted earnings per share was $2.48 and increased 22%\n",
    "“Digital technology is the most malleable resource at the world’s disposal to overcome constraints and reimagine everyday work and life,” said Satya Nadella, chairman and chief executive officer of Microsoft. “As tech as a percentage of global GDP continues to increase, we are innovating and investing across diverse and growing markets, with a common underlying technology stack and an operating model that reinforces a common strategy, culture, and sense of purpose.”\n",
    "“Solid commercial execution, represented by strong bookings growth driven by long-term Azure commitments, increased Microsoft Cloud revenue to $22.1 billion, up 32% year over year” said Amy Hood, executive vice president and chief financial officer of Microsoft.\"\"\"\n",
    "\n",
    "doc = nlp(earnings_text)\n",
    "\n",
    "filtered_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ not in [\"SPACE\", \"PUNCT\", \"X\"]:\n",
    "        filtered_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "836035bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{96: 13,\n",
       " 92: 46,\n",
       " 100: 24,\n",
       " 90: 9,\n",
       " 85: 16,\n",
       " 93: 16,\n",
       " 97: 27,\n",
       " 98: 1,\n",
       " 84: 20,\n",
       " 103: 10,\n",
       " 87: 6,\n",
       " 99: 5,\n",
       " 89: 12,\n",
       " 86: 3,\n",
       " 94: 3,\n",
       " 95: 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = doc.count_by(spacy.attrs.POS)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bf113f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN | 13\n",
      "NOUN | 46\n",
      "VERB | 24\n",
      "DET | 9\n",
      "ADP | 16\n",
      "NUM | 16\n",
      "PUNCT | 27\n",
      "SCONJ | 1\n",
      "ADJ | 20\n",
      "SPACE | 10\n",
      "AUX | 6\n",
      "SYM | 5\n",
      "CCONJ | 12\n",
      "ADV | 3\n",
      "PART | 3\n",
      "PRON | 2\n"
     ]
    }
   ],
   "source": [
    "for k,v in count.items():\n",
    "    print(doc.vocab[k].text,\"|\",v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24b1f42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARDINAL| Numerals that do not fall under another type\n",
      "DATE| Absolute or relative dates or periods\n",
      "EVENT| Named hurricanes, battles, wars, sports events, etc.\n",
      "FAC| Buildings, airports, highways, bridges, etc.\n",
      "GPE| Countries, cities, states\n",
      "LANGUAGE| Any named language\n",
      "LAW| Named documents made into laws.\n",
      "LOC| Non-GPE locations, mountain ranges, bodies of water\n",
      "MONEY| Monetary values, including unit\n",
      "NORP| Nationalities or religious or political groups\n",
      "ORDINAL| \"first\", \"second\", etc.\n",
      "ORG| Companies, agencies, institutions, etc.\n",
      "PERCENT| Percentage, including \"%\"\n",
      "PERSON| People, including fictional\n",
      "PRODUCT| Objects, vehicles, foods, etc. (not services)\n",
      "QUANTITY| Measurements, as of weight or distance\n",
      "TIME| Times smaller than a day\n",
      "WORK_OF_ART| Titles of books, songs, etc.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "tagger=nlp.get_pipe(\"ner\")\n",
    "for label in tagger.labels:\n",
    "    print(f\"{label}| {spacy.explain(label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5c72eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "71bff479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter|PRODUCT\n",
      "$45 billion|MONEY\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(\"Tesla  is going to acquire Twitter for $45 billion\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent}|{ent.label_}\")\n",
    "#suppose i have add an new NER and you also now there some buggs in ner too so to correct it we could add our own NER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f4cef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "s1=Span(doc,0,1,label=\"ORG\")\n",
    "s2=Span(doc,5,6,label=\"ORG\")\n",
    "doc.set_ents([s1,s2],default=\"unmodified\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
